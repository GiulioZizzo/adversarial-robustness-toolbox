import numpy as np
import tensorflow as tf

from typing import Optional, Union


class MalwareGD:

    def __init__(self,
                 model_weights,
                 param_dic,
                 num_of_iterations: Optional[int] = 10,
                 lf=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
                 l0: Union[float, int] = 0.1) -> None:

        """
        :param model_weights: The weights of the trained model
        :param param_dic: a dictionary specifying some MalConv parameters.
                         'maxlen': the input size to the MalConv model
                         'input_dim': the number of discrete values, normally 257.
                         'embedding_size': size of the embedding layer. Default 8.
        :param num_of_iterations: The number of iterations to apply
        :param lf: loss function used by the classifier. Here we will use CategoricalCorssentropy to fit into the
                   broader ART pattern of everything being multiclass. In future, change to also have BinaryCrossentropy
        :param l0:
        """
        self.param_dic = param_dic
        self.embedding_model = self.get_embedding_model()
        self.prediction_model = self.get_prediction_model()
        self.l0 = l0
        self.num_of_iterations = num_of_iterations
        self.global_weights = model_weights
        self.lf = lf
        self.assign_weights()

    def get_embedding_model(self):
        """
        One layer model to go from the raw data to the embeddings.
        """
        inp = tf.keras.layers.Input(shape=(self.param_dic['maxlen'],))
        emb = tf.keras.layers.Embedding(self.param_dic['input_dim'],
                                        self.param_dic['embedding_size'],
                                        name='embedding_layer')(inp)
        return tf.keras.Model(inputs=inp, outputs=emb)

    def get_prediction_model(self):
        """
        Model going from embeddings to predictions so we can easily optimise the embedding malware embedding.
        Needs to have the same structure as the target model.
        Populated here with "standard" parameters.
        """
        inp = tf.keras.layers.Input(shape=(self.param_dic['maxlen'], 8,))
        filt = tf.keras.layers.Conv1D(filters=128, kernel_size=500,
                                      strides=500, use_bias=True, activation='relu',
                                      padding='valid', name='filt_layer')(inp)
        attn = tf.keras.layers.Conv1D(filters=128, kernel_size=500,
                                      strides=500, use_bias=True, activation='sigmoid',
                                      padding='valid', name='attn_layer')(inp)
        gated = tf.keras.layers.Multiply()([filt, attn])
        feat = tf.keras.layers.GlobalMaxPooling1D()(gated)
        dense = tf.keras.layers.Dense(128, activation='relu', name='dense_layer')(feat)
        output = tf.keras.layers.Dense(2, name='output_layer')(dense)
        return tf.keras.Model(inputs=inp, outputs=[gated, output, output[:, 1]])

    def assign_weights(self):
        """
        Put the weights from the target model into the split models
        (data -> embedding model and embedding -> predictions)
        """
        embedding_weight_list = self.embedding_model.get_weights()
        prediction_model_weight_list = self.prediction_model.get_weights()

        for i, w in enumerate(self.global_weights):
            if i == 0:
                embedding_weight_list[0] = w
            else:
                prediction_model_weight_list[i-1] = w

        self.embedding_model.set_weights(embedding_weight_list)
        self.prediction_model.set_weights(prediction_model_weight_list)

    @staticmethod
    def initialise_sample(x, y, sample_sizes, pertubation_size, batch_of_slack_sizes, batch_of_slack_starts):
        """
        Randomly append bytes at the end of the malware to initialise it, or if slack regions are provided,
        perturb those.
        """
        for j in range(len(x)):
            if np.argmax(y[j]) == 1:
                if batch_of_slack_sizes is not None:
                    for sample_slack_sizes, sample_slack_starts in zip(batch_of_slack_sizes, batch_of_slack_starts):
                        for size, start in zip(sample_slack_sizes, sample_slack_starts):
                            x[j, start:start + size] = np.random.randint(low=0, high=256, size=(1, size))
                x[j, sample_sizes[j]:sample_sizes[j]+pertubation_size[j]] = np.random.randint(low=0, high=256, size=(1, pertubation_size[j]))

        return x

    def check_valid_size(self, y, sample_sizes, pertubation_size):
        """
        Checks that we can append the entire l0 pertubation to the malware sample and not exceed the
        maximum filesize

        We make here a new label vector with just the valid files indicated.
        """

        adv_label_vector = np.zeros_like(y)
        for i in range(len(y)):
            if np.argmax(y[i]) == 1:
                if sample_sizes[i] + pertubation_size[i] < self.param_dic['maxlen']:
                    adv_label_vector[i, 1] = 1
        return adv_label_vector

    @staticmethod
    def generate_mask(x, y, sample_sizes, perturbation_size,  batch_of_slack_sizes, batch_of_slack_starts):
        """
        Makes a mask we apply to the gradients to control which samples in the batch are perturbed.

        :param x: array with input data.
        :param y: labels to make sure the benign files are zero masked.
        :param sample_sizes: the size of the original file, before it was padded to the input size required by MalConv
        :param perturbation_size: size of the perturbations in L0 terms to put in slack regions/at end of file
        :param batch_of_slack_sizes: list of length batch size, each element is in itself a list containing the size of the
                              allowable slack region
        :param batch_of_slack_starts: list of length batch size, each element is in itself a list containing the start of slack
                               region.
        """
        mask = np.zeros_like(x)
        for i in range(len(x)):
            if np.argmax(y[i]) == 1:
                # if no section information was provided, append pertubation at the end of the file.
                if batch_of_slack_sizes is None:
                    mask[i, sample_sizes[i]:sample_sizes[i] + perturbation_size[i]] = 1
                else:
                    for sample_slack_sizes, sample_slack_starts in zip(batch_of_slack_sizes, batch_of_slack_starts):
                        for size, start in zip(sample_slack_sizes, sample_slack_starts):
                            mask[i, start:start + size] = 1
                    mask[i, sample_sizes[i]:sample_sizes[i] + perturbation_size[i]] = 1

        mask = np.expand_dims(mask, axis=-1)
        # repeat it so that it matches the 8 dimesnional embedding layer
        mask = np.concatenate([mask, mask, mask, mask, mask, mask, mask, mask], axis=-1)
        mask = tf.convert_to_tensor(mask)
        mask = tf.cast(mask, dtype='float32')
        return mask

    def amplify_initial_gradient(self, embeddings):
        """
        maximise the activation in the maxpool layer from our adversarial perturbations
        """
        with tf.GradientTape() as tape:
            tape.watch(embeddings)
            gated, _, _ = self.prediction_model(embeddings)
        # gradients wrt maximising activations
        gated_gradients = tape.gradient(gated, embeddings)
        return gated_gradients

    def compute_grads_to_output(self, embeddings, labels):
        with tf.GradientTape() as tape:
            tape.watch(embeddings)
            gated, output, mal_output = self.prediction_model(embeddings)
            classification_loss = self.lf(labels, output)
        loss_gradients = - tape.gradient(mal_output, embeddings)

        return loss_gradients, output, classification_loss

    @staticmethod
    def update_embeddings(embeddings, model_gradients, mask):
        embeddings = embeddings + model_gradients * mask
        return embeddings

    def get_adv_malware(self, embeddings, labels, fsize, data, num_bytes, batch_of_slack_sizes=None, batch_of_slack_starts=None):
        """
        Project the adversarial example back though the closest l2 vector
        """
        embedding_lookup = self.embedding_model.get_weights()[0]

        for i in range(len(fsize)):
            if np.argmax(labels[i]) == 1:
                m = tf.constant([1, self.param_dic['input_dim'], 1], tf.int32)

                if batch_of_slack_sizes is not None:
                    for sample_slack_sizes, sample_slack_starts in zip(batch_of_slack_sizes, batch_of_slack_starts):
                        for size, start in zip(sample_slack_sizes, sample_slack_starts):
                            expanded = tf.tile(tf.expand_dims(embeddings[i, start:start + size, :], axis=1), m)
                            diff = tf.norm((expanded - embedding_lookup), axis=-1)
                            diff = tf.math.argmin(diff, axis=-1)
                            data[i, start:start + size] = diff

                expanded = tf.tile(tf.expand_dims(embeddings[i, fsize[i]:fsize[i]+num_bytes[i], :], axis=1), m)
                diff = tf.norm((expanded - embedding_lookup), axis=-1)
                diff = tf.math.argmin(diff, axis=-1)
                data[i, fsize[i]:fsize[i]+num_bytes[i]] = diff

        return data

    @staticmethod
    def pull_out_adversarial_malware(x, y):
        """
        Helper function. Not Currently used.
        """
        adv_x = []
        for i in range(len(x)):
            if y[i, 1] == 1:
                adv_x.append(adv_x)
        return adv_x

    def check_block_selection(self, embeddings, file_sizes, num_bytes, y_adv, preds):
        """
        Function to verify how much of the adversarial perturbation is being selected
        Not currently used.
        """
        adv_block = 0
        pre_pool_result, _, _ = self.selected_block_model(embeddings)
        selected_blocks = np.argmax(np.squeeze(pre_pool_result), axis=1)
        min_block_number = file_sizes / 500
        max_block_number = (file_sizes + num_bytes) / 500
        for k in range(len(preds)):
            if y_adv[k] == 1:
                blocks = selected_blocks[k]
                for b in range(len(blocks)):
                    if blocks[b] > min_block_number[k]:
                        if blocks[b] < max_block_number[k]:
                            adv_block += 1
        print('adv block selected ', adv_block)

    def generate(self, x: np.ndarray, y: [np.ndarray, "tf.Tensor"], sample_sizes: [np.ndarray, "tf.Tensor"],
                 batch_of_slack_sizes=None, batch_of_slack_starts=None):
        """
        Makes a mask we apply to the gradients to control which samples in the batch are perturbed.

        :param x: array with input data.
        :param y: labels to make sure the benign files are zero masked. For now two class softmax labels to fit in with
                  the broader ART approach. Later change to binary classification.
        :param sample_sizes: the size of the original file, before it was padded to the input size required by MalConv
        :param batch_of_slack_sizes: list of length batch size, each element is in itself a list containing
                                     the size of the allowable slack region
        :param batch_of_slack_starts: list of length batch size, each element is in itself a list containing
                                      the start of slack region.
        """

        if np.sum(y[:, 1]) == 0:
            # no adversarial samples are in this batch
            return x, y

        pertubation_size = []
        for sample_size in sample_sizes:
            if self.l0 < 1:  # l0 is a fraction of the filesize
                pertubation_size.append(int(sample_size * self.l0))
            else:  # or l0 is interpreted as total perturbation size
                pertubation_size.append(self.l0)
        pertubation_size = np.asarray(pertubation_size)

        # reduce the perturbation we add at the end if we have slack regions
        if batch_of_slack_sizes is not None:
            for i in range(len(pertubation_size)):
                section_sizes = batch_of_slack_sizes[i]
                for size in section_sizes:
                    pertubation_size[i] = pertubation_size[i] - size
            pertubation_size = np.where(pertubation_size < 0, 0, pertubation_size)

        y = self.check_valid_size(y, sample_sizes, pertubation_size)

        x = self.initialise_sample(x, y, sample_sizes, pertubation_size,
                                   batch_of_slack_sizes=batch_of_slack_sizes,
                                   batch_of_slack_starts=batch_of_slack_starts)

        mask = self.generate_mask(x, y, sample_sizes, pertubation_size,
                                  batch_of_slack_sizes=batch_of_slack_sizes, batch_of_slack_starts=batch_of_slack_starts)

        embeddings = self.embedding_model(x)

        model_gradients = self.amplify_initial_gradient(embeddings)

        embeddings = self.update_embeddings(embeddings, model_gradients, mask)

        for _ in range(self.num_of_iterations):
            model_gradients, _, _ = self.compute_grads_to_output(embeddings, y)
            embeddings = self.update_embeddings(embeddings, model_gradients, mask)

        x = self.get_adv_malware(embeddings, y, sample_sizes, x, pertubation_size,
                                 batch_of_slack_sizes=batch_of_slack_sizes, batch_of_slack_starts=batch_of_slack_starts)

        return x, y

    @staticmethod
    def process_file(filepath, padding_char=256, maxlen=2**20):

        """
        Go from raw file to numpy array.
            maxlen: maximum size of the file processed by the model. Currently set to 1MB
            padding_char: char to use to pad the input if it is shorter then maxlen

        returns: x: numpy array of data
                 filesize: size of original file
        """

        f = open(filepath, "rb").read()
        size_of_original_file = len(f)

        b = np.ones((maxlen,), dtype=np.uint16) * padding_char
        bytez = np.frombuffer(f[:maxlen], dtype=np.uint8)
        b[:len(bytez)] = bytez

        return b, size_of_original_file

    @staticmethod
    def get_peinfo(filepath, save_to_json=False, save_to_json_path=None):

        """
        Given a PE file we extract out the section information to determine the slack regions in the file.
        We return two lists 1) with the start location of the slack regions and 2) with the size of the slack region.
        Here we are using pedump to get the information (https://github.com/zed-0xff/pedump)
        """
        import os
        import json

        cmd = 'pedump --format json --sections ' + filepath
        result = os.popen(cmd).read()

        # If the PE file could not be read we return empty lists
        if not result:
            return [], []

        # parse the output of pedump to make it into a nice json
        result = result[19:]
        tmp = json.loads(result)

        cleaned_dump = {}
        for entry in tmp:
            entry = entry.replace(' ', '')
            entry = entry.split(",")

            section_name = entry[0]
            entry = entry[1:]
            section_info = {}
            for sec in entry:
                sec = sec.split('=')
                section_info[sec[0]] = sec[1]

            cleaned_dump[section_name] = section_info

        if save_to_json:
            with open(os.path.join(save_to_json_path, 'pe_dump.json'), 'w') as outfile:
                json.dump(cleaned_dump, outfile, indent=4, sort_keys=True)

        size_of_slack = []
        start_of_slack = []
        for section_name, section in cleaned_dump.items():
            print(section_name)
            raw_data_size = int(section['SizeOfRawData'])
            virtual_data_size = int(section['VirtualSize'])
            raw_address = int(section['PointerToRawData'])

            slack = raw_data_size - virtual_data_size

            if slack > 0:
                size_of_slack.append(slack)
                start_of_slack.append(raw_address+virtual_data_size)

        return size_of_slack, start_of_slack